{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_training_multi_class_text_classifier_demo.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO3jZVOc7zYe5aEZ0k0kOeW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zkufh760uvF3"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/JohnSnowLabs/nlu/blob/master/examples/collab/Training/NLU_training_multi_class_text_classifier_demo.ipynb)\n","\n","\n","\n","# Training a Deep Learning Classifier with NLU \n","## ClassifierDL (Multi-class Text Classification)\n","With the [ClassifierDL model](https://nlp.johnsnowlabs.com/docs/en/annotators#classifierdl-multi-class-text-classification) from Spark NLP you can achieve State Of the Art results on any multi class text classification problem \n","\n","This notebook showcases the following features : \n","\n","- How to train the deep learning classifier\n","- How to store a pipeline to disk\n","- How to load the pipeline from disk (Enables NLU offline mode)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dur2drhW5Rvi"},"source":["# 1. Install Java 8 and NLU"]},{"cell_type":"code","metadata":{"id":"hFGnBCHavltY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606717706664,"user_tz":-60,"elapsed":79367,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"eaac7eae-e137-4de7-b659-fd39efcad57c"},"source":["import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","# ! pip install nlu > /dev/null\n","! pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple peanutbutterdatatime==1.0.4rc11\n","\n","import nlu"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple\n","Collecting peanutbutterdatatime==1.0.4rc11\n","\u001b[?25l  Downloading https://test-files.pythonhosted.org/packages/53/aa/e1f8a329dc9e9dd9fc9cbcbd2373c9c98dbd05443b9259c53537c3ad2f65/peanutbutterdatatime-1.0.4rc11-py3-none-any.whl (158kB)\n","\u001b[K     |████████████████████████████████| 163kB 3.8MB/s \n","\u001b[?25hCollecting pyarrow>=0.16.0\n","\u001b[?25l  Downloading https://test-files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n","\u001b[K     |████████████████████████████████| 17.7MB 125kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from peanutbutterdatatime==1.0.4rc11) (1.18.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from peanutbutterdatatime==1.0.4rc11) (1.1.4)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from peanutbutterdatatime==1.0.4rc11) (0.8)\n","Collecting pyspark<2.5,>=2.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/06/29f80e5a464033432eedf89924e7aa6ebbc47ce4dcd956853a73627f2c07/pyspark-2.4.7.tar.gz (217.9MB)\n","\u001b[K     |████████████████████████████████| 217.9MB 59kB/s \n","\u001b[?25hCollecting spark-nlp<2.7,>=2.6.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/26/f7a6ac12339d2f1ed271c46c16705665620059e4559f323695925f3c63b4/spark_nlp-2.6.4-py2.py3-none-any.whl (129kB)\n","\u001b[K     |████████████████████████████████| 133kB 45.5MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->peanutbutterdatatime==1.0.4rc11) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->peanutbutterdatatime==1.0.4rc11) (2018.9)\n","Collecting py4j==0.10.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n","\u001b[K     |████████████████████████████████| 204kB 49.0MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->peanutbutterdatatime==1.0.4rc11) (1.15.0)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-2.4.7-py2.py3-none-any.whl size=218279466 sha256=9849b3166809f2a96296f9d46c5f19e9ca889c411061576d379099d4437dac05\n","  Stored in directory: /root/.cache/pip/wheels/34/1f/2e/1e7460f80acf26b08dbb8c53d7ff9e07146f2a68dd5c732be5\n","Successfully built pyspark\n","Installing collected packages: pyarrow, py4j, pyspark, spark-nlp, peanutbutterdatatime\n","  Found existing installation: pyarrow 0.14.1\n","    Uninstalling pyarrow-0.14.1:\n","      Successfully uninstalled pyarrow-0.14.1\n","Successfully installed peanutbutterdatatime-1.0.4rc11 py4j-0.10.7 pyarrow-2.0.0 pyspark-2.4.7 spark-nlp-2.6.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f4KkTfnR5Ugg"},"source":["# 2. Download news classification dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OrVb5ZMvvrQD","executionInfo":{"status":"ok","timestamp":1606717712703,"user_tz":-60,"elapsed":85396,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"934ac84b-469f-46bc-d5c7-8fb2f648cf84"},"source":["! wget https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_train.csv\n","! wget https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2020-11-30 06:28:26--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_train.csv\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.154.126\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.154.126|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 24032125 (23M) [text/csv]\n","Saving to: ‘news_category_train.csv’\n","\n","news_category_train 100%[===================>]  22.92M  9.04MB/s    in 2.5s    \n","\n","2020-11-30 06:28:29 (9.04 MB/s) - ‘news_category_train.csv’ saved [24032125/24032125]\n","\n","--2020-11-30 06:28:29--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.154.126\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.154.126|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1504408 (1.4M) [text/csv]\n","Saving to: ‘news_category_test.csv’\n","\n","news_category_test. 100%[===================>]   1.43M  1.32MB/s    in 1.1s    \n","\n","2020-11-30 06:28:31 (1.32 MB/s) - ‘news_category_test.csv’ saved [1504408/1504408]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"y4xSRWIhwT28","executionInfo":{"status":"ok","timestamp":1606717712704,"user_tz":-60,"elapsed":85389,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"e0bc179e-6139-4596-c7c8-cdc17f8dad77"},"source":["import pandas as pd\n","test_path = '/content/news_category_test.csv'\n","train_df = pd.read_csv(test_path)\n","train_df.columns=['y','text']\n","train_df"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>y</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Business</td>\n","      <td>Unions representing workers at Turner   Newall...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sci/Tech</td>\n","      <td>TORONTO, Canada    A second team of rocketeer...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sci/Tech</td>\n","      <td>A company founded by a chemistry researcher a...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sci/Tech</td>\n","      <td>It's barely dawn when Mike Fitzpatrick starts...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sci/Tech</td>\n","      <td>Southern California's smog fighting agency we...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7595</th>\n","      <td>World</td>\n","      <td>Ukrainian presidential candidate Viktor Yushch...</td>\n","    </tr>\n","    <tr>\n","      <th>7596</th>\n","      <td>Sports</td>\n","      <td>With the supply of attractive pitching options...</td>\n","    </tr>\n","    <tr>\n","      <th>7597</th>\n","      <td>Sports</td>\n","      <td>Like Roger Clemens did almost exactly eight ye...</td>\n","    </tr>\n","    <tr>\n","      <th>7598</th>\n","      <td>Business</td>\n","      <td>SINGAPORE : Doctors in the United States have ...</td>\n","    </tr>\n","    <tr>\n","      <th>7599</th>\n","      <td>Business</td>\n","      <td>EBay plans to buy the apartment and home renta...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7600 rows × 2 columns</p>\n","</div>"],"text/plain":["             y                                               text\n","0     Business  Unions representing workers at Turner   Newall...\n","1     Sci/Tech   TORONTO, Canada    A second team of rocketeer...\n","2     Sci/Tech   A company founded by a chemistry researcher a...\n","3     Sci/Tech   It's barely dawn when Mike Fitzpatrick starts...\n","4     Sci/Tech   Southern California's smog fighting agency we...\n","...        ...                                                ...\n","7595     World  Ukrainian presidential candidate Viktor Yushch...\n","7596    Sports  With the supply of attractive pitching options...\n","7597    Sports  Like Roger Clemens did almost exactly eight ye...\n","7598  Business  SINGAPORE : Doctors in the United States have ...\n","7599  Business  EBay plans to buy the apartment and home renta...\n","\n","[7600 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"0296Om2C5anY"},"source":["# 3. Train Deep Learning Classifier using nlu.load('train.classifier')\n","\n","By default, the Universal Sentence Encoder Embeddings (USE) are beeing downloaded to provide embeddings for the classifier. You can use any of the 50+ other sentence Emeddings in NLU tough!\n","\n","You dataset label column should be named 'y' and the feature column with text data should be named 'text'"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"3ZIPkRkWftBG","executionInfo":{"status":"ok","timestamp":1606718168168,"user_tz":-60,"elapsed":540845,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"14936984-5b05-4e70-e59b-9eaa4d98a403"},"source":["# load a trainable pipeline by specifying the train. prefix  and fit it on a datset with label and text columns\n","# Since there are no\n","fitted_pipe = nlu.load('train.classifier').fit(train_df)\n","\n","# predict with the trainable pipeline on dataset and get predictions\n","preds = fitted_pipe.predict(train_df)\n","preds"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>category</th>\n","      <th>y</th>\n","      <th>text</th>\n","      <th>default_name_embeddings</th>\n","      <th>category_confidence</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Unions representing workers at Turner Newall s...</td>\n","      <td>Business</td>\n","      <td>Business</td>\n","      <td>Unions representing workers at Turner   Newall...</td>\n","      <td>[0.012997539713978767, 0.019844762980937958, -...</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TORONTO, Canada A second team of rocketeers co...</td>\n","      <td>Sports</td>\n","      <td>Sci/Tech</td>\n","      <td>TORONTO, Canada    A second team of rocketeer...</td>\n","      <td>[0.023022323846817017, -0.01595703884959221, -...</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10 million Ansari X Prize, a contest for priva...</td>\n","      <td>Sports</td>\n","      <td>Sci/Tech</td>\n","      <td>TORONTO, Canada    A second team of rocketeer...</td>\n","      <td>[-0.010587693192064762, 0.011531050316989422, ...</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A company founded by a chemistry researcher at...</td>\n","      <td>Sci/Tech</td>\n","      <td>Sci/Tech</td>\n","      <td>A company founded by a chemistry researcher a...</td>\n","      <td>[0.038641855120658875, 0.02322080172598362, -0...</td>\n","      <td>0.995407</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>It's barely dawn when Mike Fitzpatrick starts ...</td>\n","      <td>Sci/Tech</td>\n","      <td>Sci/Tech</td>\n","      <td>It's barely dawn when Mike Fitzpatrick starts...</td>\n","      <td>[-0.006857294123619795, 0.01967567577958107, -...</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7596</th>\n","      <td>.</td>\n","      <td>Sports</td>\n","      <td>Sports</td>\n","      <td>With the supply of attractive pitching options...</td>\n","      <td>[0.005107458680868149, -0.011805553920567036, ...</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7596</th>\n","      <td>.</td>\n","      <td>Sports</td>\n","      <td>Sports</td>\n","      <td>With the supply of attractive pitching options...</td>\n","      <td>[0.005107458680868149, -0.011805553920567036, ...</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7597</th>\n","      <td>Like Roger Clemens did almost exactly eight ye...</td>\n","      <td>Sports</td>\n","      <td>Sports</td>\n","      <td>Like Roger Clemens did almost exactly eight ye...</td>\n","      <td>[0.044696468859910965, 0.0015660696662962437, ...</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7598</th>\n","      <td>SINGAPORE : Doctors in the United States have ...</td>\n","      <td>Business</td>\n","      <td>Business</td>\n","      <td>SINGAPORE : Doctors in the United States have ...</td>\n","      <td>[0.05564942583441734, -0.021285761147737503, -...</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7599</th>\n","      <td>EBay plans to buy the apartment and home renta...</td>\n","      <td>Business</td>\n","      <td>Business</td>\n","      <td>EBay plans to buy the apartment and home renta...</td>\n","      <td>[0.08172684907913208, -0.013251541182398796, -...</td>\n","      <td>0.999687</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14399 rows × 6 columns</p>\n","</div>"],"text/plain":["                                                       sentence  ... category_confidence\n","origin_index                                                     ...                    \n","0             Unions representing workers at Turner Newall s...  ...            1.000000\n","1             TORONTO, Canada A second team of rocketeers co...  ...            1.000000\n","1             10 million Ansari X Prize, a contest for priva...  ...            1.000000\n","2             A company founded by a chemistry researcher at...  ...            0.995407\n","3             It's barely dawn when Mike Fitzpatrick starts ...  ...            1.000000\n","...                                                         ...  ...                 ...\n","7596                                                          .  ...            1.000000\n","7596                                                          .  ...            2.000000\n","7597          Like Roger Clemens did almost exactly eight ye...  ...            1.000000\n","7598          SINGAPORE : Doctors in the United States have ...  ...            1.000000\n","7599          EBay plans to buy the apartment and home renta...  ...            0.999687\n","\n","[14399 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"DL_5aY9b3jSd"},"source":["# 4. Evaluate the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djtoZVKBw2WU","executionInfo":{"status":"ok","timestamp":1606718169584,"user_tz":-60,"elapsed":542253,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"a7851918-c217-41e9-abc7-4f1b57000141"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(preds['y'], preds['category']))\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    Business       0.81      0.82      0.82      3671\n","    Sci/Tech       0.83      0.84      0.83      3983\n","      Sports       0.88      0.94      0.91      3687\n","       World       0.91      0.80      0.85      3058\n","\n","    accuracy                           0.85     14399\n","   macro avg       0.86      0.85      0.85     14399\n","weighted avg       0.85      0.85      0.85     14399\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mhFKVN93o1ZO"},"source":["# 5. Lets try different Sentence Emebddings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzJd8omao0gt","executionInfo":{"status":"ok","timestamp":1606718169585,"user_tz":-60,"elapsed":542247,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"7068d53e-9fce-48ba-a5f2-1afbfd248bad"},"source":["# We can use nlu.print_components(action='embed_sentence') to see every possibler sentence embedding we could use. Lets use bert!\n","nlu.print_components(action='embed_sentence')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["For language <en> NLU provides the following Models : \n","nlu.load('en.embed_sentence') returns Spark NLP model tfhub_use\n","nlu.load('en.embed_sentence.use') returns Spark NLP model tfhub_use\n","nlu.load('en.embed_sentence.tfhub_use') returns Spark NLP model tfhub_use\n","nlu.load('en.embed_sentence.use.lg') returns Spark NLP model tfhub_use_lg\n","nlu.load('en.embed_sentence.tfhub_use.lg') returns Spark NLP model tfhub_use_lg\n","nlu.load('en.embed_sentence.albert') returns Spark NLP model albert_base_uncased\n","nlu.load('en.embed_sentence.electra') returns Spark NLP model sent_electra_small_uncased\n","nlu.load('en.embed_sentence.electra_small_uncased') returns Spark NLP model sent_electra_small_uncased\n","nlu.load('en.embed_sentence.electra_base_uncased') returns Spark NLP model sent_electra_base_uncased\n","nlu.load('en.embed_sentence.electra_large_uncased') returns Spark NLP model sent_electra_large_uncased\n","nlu.load('en.embed_sentence.bert') returns Spark NLP model sent_bert_base_uncased\n","nlu.load('en.embed_sentence.bert_base_uncased') returns Spark NLP model sent_bert_base_uncased\n","nlu.load('en.embed_sentence.bert_base_cased') returns Spark NLP model sent_bert_base_cased\n","nlu.load('en.embed_sentence.bert_large_uncased') returns Spark NLP model sent_bert_large_uncased\n","nlu.load('en.embed_sentence.bert_large_cased') returns Spark NLP model sent_bert_large_cased\n","nlu.load('en.embed_sentence.biobert.pubmed_base_cased') returns Spark NLP model sent_biobert_pubmed_base_cased\n","nlu.load('en.embed_sentence.biobert.pubmed_large_cased') returns Spark NLP model sent_biobert_pubmed_large_cased\n","nlu.load('en.embed_sentence.biobert.pmc_base_cased') returns Spark NLP model sent_biobert_pmc_base_cased\n","nlu.load('en.embed_sentence.biobert.pubmed_pmc_base_cased') returns Spark NLP model sent_biobert_pubmed_pmc_base_cased\n","nlu.load('en.embed_sentence.biobert.clinical_base_cased') returns Spark NLP model sent_biobert_clinical_base_cased\n","nlu.load('en.embed_sentence.biobert.discharge_base_cased') returns Spark NLP model sent_biobert_discharge_base_cased\n","nlu.load('en.embed_sentence.covidbert.large_uncased') returns Spark NLP model sent_covidbert_large_uncased\n","nlu.load('en.embed_sentence.small_bert_L2_128') returns Spark NLP model sent_small_bert_L2_128\n","nlu.load('en.embed_sentence.small_bert_L4_128') returns Spark NLP model sent_small_bert_L4_128\n","nlu.load('en.embed_sentence.small_bert_L6_128') returns Spark NLP model sent_small_bert_L6_128\n","nlu.load('en.embed_sentence.small_bert_L8_128') returns Spark NLP model sent_small_bert_L8_128\n","nlu.load('en.embed_sentence.small_bert_L10_128') returns Spark NLP model sent_small_bert_L10_128\n","nlu.load('en.embed_sentence.small_bert_L12_128') returns Spark NLP model sent_small_bert_L12_128\n","nlu.load('en.embed_sentence.small_bert_L2_256') returns Spark NLP model sent_small_bert_L2_256\n","nlu.load('en.embed_sentence.small_bert_L4_256') returns Spark NLP model sent_small_bert_L4_256\n","nlu.load('en.embed_sentence.small_bert_L6_256') returns Spark NLP model sent_small_bert_L6_256\n","nlu.load('en.embed_sentence.small_bert_L8_256') returns Spark NLP model sent_small_bert_L8_256\n","nlu.load('en.embed_sentence.small_bert_L10_256') returns Spark NLP model sent_small_bert_L10_256\n","nlu.load('en.embed_sentence.small_bert_L12_256') returns Spark NLP model sent_small_bert_L12_256\n","nlu.load('en.embed_sentence.small_bert_L2_512') returns Spark NLP model sent_small_bert_L2_512\n","nlu.load('en.embed_sentence.small_bert_L4_512') returns Spark NLP model sent_small_bert_L4_512\n","nlu.load('en.embed_sentence.small_bert_L6_512') returns Spark NLP model sent_small_bert_L6_512\n","nlu.load('en.embed_sentence.small_bert_L8_512') returns Spark NLP model sent_small_bert_L8_512\n","nlu.load('en.embed_sentence.small_bert_L10_512') returns Spark NLP model sent_small_bert_L10_512\n","nlu.load('en.embed_sentence.small_bert_L12_512') returns Spark NLP model sent_small_bert_L12_512\n","nlu.load('en.embed_sentence.small_bert_L2_768') returns Spark NLP model sent_small_bert_L2_768\n","nlu.load('en.embed_sentence.small_bert_L4_768') returns Spark NLP model sent_small_bert_L4_768\n","nlu.load('en.embed_sentence.small_bert_L6_768') returns Spark NLP model sent_small_bert_L6_768\n","nlu.load('en.embed_sentence.small_bert_L8_768') returns Spark NLP model sent_small_bert_L8_768\n","nlu.load('en.embed_sentence.small_bert_L10_768') returns Spark NLP model sent_small_bert_L10_768\n","nlu.load('en.embed_sentence.small_bert_L12_768') returns Spark NLP model sent_small_bert_L12_768\n","For language <fi> NLU provides the following Models : \n","nlu.load('fi.embed_sentence') returns Spark NLP model sent_bert_finnish_cased\n","nlu.load('fi.embed_sentence.bert.cased') returns Spark NLP model sent_bert_finnish_cased\n","nlu.load('fi.embed_sentence.bert.uncased') returns Spark NLP model sent_bert_finnish_uncased\n","For language <xx> NLU provides the following Models : \n","nlu.load('xx.embed_sentence') returns Spark NLP model sent_bert_multi_cased\n","nlu.load('xx.embed_sentence.bert') returns Spark NLP model sent_bert_multi_cased\n","nlu.load('xx.embed_sentence.bert.cased') returns Spark NLP model sent_bert_multi_cased\n","nlu.load('xx.embed_sentence.labse') returns Spark NLP model labse\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ABHLgirmG1n9","outputId":"9e36f030-6da4-4622-bb24-70e0f64c9ecc"},"source":["# Load pipe with bert embeds\n","# using large embeddings can take a few hours..\n","# fitted_pipe = nlu.load('en.embed_sentence.bert_large_uncased train.classifier').fit(train_df)\n","fitted_pipe = nlu.load('en.embed_sentence.small_bert_L12_768 train.classifier').fit(train_df)\n","\n","\n","# predict with the trained pipeline on dataset and get predictions\n","preds = fitted_pipe.predict(train_df)\n","from sklearn.metrics import classification_report\n","print(classification_report(preds['y'], preds['category']))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sent_small_bert_L12_768 download started this may take some time.\n","Approximate size to download 392.9 MB\n","[OK!]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nbpdZGoZPslz"},"source":["# Load pipe with bert embeds\n","fitted_pipe = nlu.load('embed_sentence.bert train.classifier').fit(train_df)\n","\n","# predict with the trained pipeline on dataset and get predictions\n","preds = fitted_pipe.predict(train_df)\n","from sklearn.metrics import classification_report\n","print(classification_report(preds['y'], preds['category']))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2BB-NwZUoHSe"},"source":["# 5. Lets save the model"]},{"cell_type":"code","metadata":{"id":"eLex095goHwm"},"source":["stored_model_path = './models/classifier_dl_trained' \n","fitted_pipe.save(stored_model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_b2DPd4rCiU"},"source":["# 6. Lets load the model from HDD.\n","This makes Offlien NLU usage possible!   \n","You need to call nlu.load(path=path_to_the_pipe) to load a model/pipeline from disk."]},{"cell_type":"code","metadata":{"id":"SO4uz45MoRgp"},"source":["hdd_pipe = nlu.load(path=stored_model_path)\n","\n","preds = hdd_pipe.predict('Tesla plans to invest 10M into the ML sector')\n","preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e0CVlkk9v6Qi"},"source":["hdd_pipe.print_info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1LjAwJVJxun"},"source":[""],"execution_count":null,"outputs":[]}]}