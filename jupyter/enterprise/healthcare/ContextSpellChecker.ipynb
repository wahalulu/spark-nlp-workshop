{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6M4RWIf4WmWq"
   },
   "source": [
    "# Context Spell Checker\n",
    "\n",
    "## Example for ContextSpellChecker\n",
    "\n",
    "\n",
    "Lets imagine we have this sentence with a couple of spelling errors (in red):\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "    I <span style=\"color: red\">habe</span> four in my family: Dad, Mum and <span style=\"color: red\">sisster</span>.<br>\n",
    "</div>\n",
    "\n",
    "SparkNLP Enterprise version provides with a pretrained SpellChecker model that can fix those errors by using contextual information. This notebook provide an example of how to use this Annotator in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJtWKirgWmWq"
   },
   "source": [
    "### Step 1. Prepare the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CcMZBzWjWwxh",
    "outputId": "57d0fc12-a04b-4fea-ed13-18a32ae5b48b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['version', 'jsl_version', 'secret', 'SPARK_NLP_LICENSE', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'JSL_OCR_LICENSE', 'JSL_OCR_SECRET'])"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('keys.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "license_keys.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "2bxdT2o-WmWr",
    "outputId": "6dcaeaa6-b598-438c-9a29-30cabb56c49c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.johnsnowlabs.com/scwgF2mD1U\n",
      "Collecting spark-nlp-jsl==2.5.4rc2\n",
      "Collecting pyspark==2.4.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 53kB/s \n",
      "\u001b[?25hCollecting spark-nlp==2.5.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/99/7a306dd04623ae25d2bd53a190c0b695fc72043773d5ae0870b7aa53d8e2/spark_nlp-2.5.4-py2.py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 38.1MB/s \n",
      "\u001b[?25hCollecting py4j==0.10.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 38.7MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: spark-nlp-jsl, pyspark\n",
      "  Building wheel for spark-nlp-jsl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for spark-nlp-jsl: filename=spark_nlp_jsl-2.5.4rc2-cp36-none-any.whl size=22983 sha256=5e31f6767f852e086edae2cb7f95386844a2f15381683a657fc9a8f93fda6dec\n",
      "  Stored in directory: /root/.cache/pip/wheels/17/aa/21/fd766748d93cdb7e75f311749ee5e90cc531837704e182302e\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130388 sha256=3d4fd962aff538eea7c4329384a5b7d937359bd25d14b6722c58021c271ff5aa\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
      "Successfully built spark-nlp-jsl pyspark\n",
      "Installing collected packages: py4j, pyspark, spark-nlp, spark-nlp-jsl\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4 spark-nlp-2.5.4 spark-nlp-jsl-2.5.4rc2\n",
      "2.5.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "secret = license_keys.get(\"secret\",license_keys.get('SPARK_NLP_SECRET', \"\"))\n",
    "spark_version = os.environ.get(\"SPARK_VERSION\", license_keys.get(\"SPARK_VERSION\",\"2.4\"))\n",
    "version = license_keys.get(\"version\",license_keys.get('SPARK_NLP_PUBLIC_VERSION', \"\"))\n",
    "jsl_version = license_keys.get(\"jsl_version\",license_keys.get('SPARK_NLP_VERSION', \"\"))\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['JSL_OCR_LICENSE'] = license_keys['JSL_OCR_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID']= license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "print(spark_version, version, jsl_version)\n",
    "\n",
    "! python -m pip install \"pyspark==$spark_version\".*\n",
    "! python -m pip install --upgrade spark-nlp-jsl==$jsl_version  --extra-index-url https://pypi.johnsnowlabs.com/$secret\n",
    "\n",
    "import sparknlp\n",
    "import sparknlp_jsl\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print (sparknlp.version())\n",
    "print (sparknlp_jsl.version())\n",
    "\n",
    "spark = sparknlp_jsl.start(secret, gpu=False, spark23=(spark_version[:3]==\"2.3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XRLYy6j4WmW5"
   },
   "source": [
    "### Step 2: Context SpellChecker pipeline generation\n",
    "\n",
    "In Spark-NLP annotating NLP happens through pipelines. Pipelines are made out of various Annotator steps. In our case the architecture of the Context SpellChecker pipeline will be:\n",
    "\n",
    "* DocumentAssembler (text -> document)\n",
    "* SentenceDetector (document -> sentence)\n",
    "* Tokenizer (sentence -> token)\n",
    "* ContextSpellCheckerModel (token -> fixed)\n",
    "\n",
    "From the original text we generate a document and identify the different sentences. For each sentence the pipeline will extract the list of tokens and feed those to the context spellchecker.\n",
    "\n",
    "Finally we will use a pretrained model (ContextSpellCheckerModel) that is trained to fix mispelling errors based on contextual information.\n",
    "\n",
    "So from a text we will end having a list of tokens spellchecked in the \"fixed\" column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZij_3YsWmW6"
   },
   "source": [
    "#### Step 2.1: Initialize all the components of the pipeline\n",
    "The first three components are pretty straightforward Transformers/Annotators: DocumentAssembler, SentenceDetector and Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8gZsEG8WmW7"
   },
   "outputs": [],
   "source": [
    "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
    "\n",
    "document = DocumentAssembler()\\\n",
    ".setInputCol(\"text\")\\\n",
    ".setOutputCol(\"document\")\n",
    "\n",
    "# Rule based Sentence Detector annotator, processes various sentences per line\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "\n",
    "token = Tokenizer()\\\n",
    ".setInputCols([\"sentence\"])\\\n",
    ".setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "rFRQqHA8WmW_",
    "outputId": "3ee4c537-1eeb-4e17-e7ba-ed72fc642782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_clinical download started this may take some time.\n",
      "Approximate size to download 145 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "spellchecker_dl = ContextSpellCheckerModel.pretrained('spellcheck_clinical', 'en', 'clinical/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "seMEH5AkWmXE"
   },
   "source": [
    "We will also setup the name of the input column (\"token\", that is the output of the previous Annotator) and output column (\"fixed\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_UgMVERWmXE"
   },
   "outputs": [],
   "source": [
    "spellchecker_dl = spellchecker_dl.setInputCols([\"token\"])\\\n",
    ".setOutputCol(\"fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOsuiC1YWmXH"
   },
   "source": [
    "#### Step 2.2 Defining the stages of the pipeline\n",
    "Now that we have created all the components of our pipeline, lets put all them together into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIioAbsIWmXH"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([\n",
    "    document,\n",
    "    sentenceDetector,\n",
    "    token,\n",
    "    spellchecker_dl\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1a-mUMpWmXK"
   },
   "source": [
    "### Step 3 Get your fitted model\n",
    "Now is time to fit our new pipeline. First we will create a Spark DataFrame including the sentence we want our SpellChecker to fix:\n",
    "\n",
    "<div style=\"border:2px solid #747474; background-color: #e3e3e3; margin: 5px; padding: 10px\"> \n",
    "    I <span style=\"color: red\">habe</span> four in my family: Dad, Mum and <span style=\"color: red\">sisster</span>.<br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "OERjy1dhWmXK",
    "outputId": "609d4184-4b02-4d23-dfd6-2367731b45c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|text                         |\n",
      "+-----------------------------+\n",
      "|I habe kancer in my left lunb|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame([\n",
    "    [\"I habe kancer in my left lunb\"]\n",
    "]).toDF('text')\n",
    "\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kCz0vdyWmXN"
   },
   "source": [
    "Now we will create a model by fitting our pipeline to our content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00EFDol9WmXO"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lqoPRIQWmXQ"
   },
   "source": [
    "### Step 4. Transform your data with the model to fix spelling errors.\n",
    "We will now apply the model transforming our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENv3nRn-WmXR"
   },
   "outputs": [],
   "source": [
    "output = model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZAwpSWIWmXT"
   },
   "source": [
    "As a result we will have a Spark DataFrame with a column containing the original tokens (\"token\") and another column with the fixed tokens (\"fixed\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "3TeeKM97WmXU",
    "outputId": "bfb9f508-4c1f-4057-c157-45d76e047891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|               fixed|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|I habe kancer in ...|[[document, 0, 28...|[[document, 0, 28...|[[token, 0, 0, I,...|[[token, 0, 0, I,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYV7IR6kWmXW"
   },
   "source": [
    "Lets compare both and see how our ContextSpellChecker has fixed the mispells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bwXsubc-WmXX",
    "outputId": "ac6b3634-843c-4d42-eb7e-f449fee79f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I habe kancer in my left lunb\n",
      "I have cancer in my left lung\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(output.select('token.result').take(1)[0]['result']))\n",
    "print(\" \".join(output.select('fixed.result').take(1)[0]['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ta4ixvNxWmXZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ContextSpellChecker.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
